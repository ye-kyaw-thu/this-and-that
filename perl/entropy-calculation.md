# Entropy Calculation

Entropy ဆိုတာက information theory မှာ အရေးကြီးတဲ့ တွက်ချက်မှုတစ်ခုပါ။ Entropy ဆိုတဲ့ သိပ္ပံဆိုင်ရင်ရာ အတွေးအခေါ်ကို ပထမဆုံး propose လုပ်ခဲ့တဲ့သူက information theory ရဲ့ ဖခင်လို့ တင်စားပြောကြတဲ့ ဒေါက်တာ Claude Elwood Shannon ပါ။ အဲဒါကြောင့်မို့လို့ Shannon Entropy လို့လည်း သုံးကြပါတယ်။ စာတမ်းက ၁၉၄၈ခုနှစ်မှာ ထုတ်ဝေခဲ့ပြီး "A Mathematical Theory of Communication" ဆိုတဲ့ ခေါင်းစဉ်နဲ့ပါ။ ကွန်ပျူတာလောကမှာ ဒေတာတွေကို သိမ်းဖို့ ပို့ဖို့ (i.e. communication) လုပ်တဲ့အခါမှာ သုံးကြတဲ့ 0, 1 (bits) ဆိုတာကလည်း ဒီဆရာကြီးရဲ့ သုတေသန ကနေ လာတာပါပဲ။     

Wiki မှာတော့ entorpy ဆိုတာကို အင်္ဂလိပ်လိုတော့ အောက်ပါအတိုင်း ရှင်းပြထားပါတယ်။  

In information theory, the entropy of a random variable is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible outcomes.  

ကွန်ပျူတာသမားတွေအတွက် နားလည်သလို ပြန်ရှင်းပြရရင် communication လုပ်မယ့် (ဥပမာ။ ။ အဲဒီခေတ်အချိန်ကအတိုင်း ပြောရရင် တနေရာကနေ တနေရာကို စာကြောင်းတစ်ကြောင်း ပိုတာမျိုး ...) စာကြောင်းကို encode လုပ်ဖို့အတွက် ပျမ်းမျှ bit အနည်းဆုံး ဘယ်လောက် လိုအပ်မလဲ ဆိုတာကို ခန့်မှန်းတွက်ချက်တာပါ။  

## Reference

1. https://en.wikipedia.org/wiki/Claude_Shannon
2. https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf

